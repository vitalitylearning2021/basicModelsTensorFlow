# -*- coding: utf-8 -*-
"""TF_Logistic_Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SBeCT4cOZtea1zwajsrnSG1pTPH3_GTO
"""

import tensorflow as tf
import numpy as np

import matplotlib.pyplot as plt

alpha               = 0.01    # --- Learning rate
numIter             = 1000    # --- Number of gradient descent iterations
skipIter            = 50      # --- Number of iterations to skip for presenting the results
shuffleBufferSize   = 5000    # --- Buffer size for shuffling the input data 
batchSize           = 256     # --- Batch size for the train data

numClasses          = 10      # --- Number of classes of the MNIST dataset (0 to 9 digits)
numFeatures         = 784     # --- Number of features (each image is a 28 x 28 image, 
                              #     to be flattened to a row of 784 elements)
                        
# --- Load MNIST dataset
from tensorflow.keras.datasets import mnist
(xTrain, yTrain), (xTest, yTest) = mnist.load_data()

# --- The dataset (xTrain, xTest, yTrain, yTest) has uint8 type. We need to cast it to float32.
xTrain, xTest = np.array(xTrain, np.float32), np.array(xTest, np.float32)

# --- The images are 28 x 28, so we flatten them to 1-D vectors of 784 features.
xTrain, xTest = xTrain.reshape([-1, numFeatures]), xTest.reshape([-1, numFeatures])

# --- The image values range from 0 to 255. We normalize them to [0, 1].
xTrain, xTest = xTrain / 255., xTest / 255.

# --- Slices the data row-wise and takes batches out of them
trainData = tf.data.Dataset.from_tensor_slices((xTrain, yTrain))
trainData = trainData.repeat().shuffle(shuffleBufferSize).batch(batchSize).prefetch(1)

# --- Defining the weights matrix.
W = tf.Variable(tf.ones([numFeatures, numClasses]))
# --- Defining the bias vector.
b = tf.Variable(tf.zeros([numClasses]))

# --- Logistic regression function (Wx + b).
def logisticRegression(x):
    return tf.nn.softmax(tf.matmul(x, W) + b)

# --- Cost function
def costFunction(pVector, yOverbar):
    # --- Encoding class label to a one hot vector.
    yOverbar = tf.one_hot(yOverbar, depth = numClasses)
    # --- Clip pVector to avoid log(0) error.
    pVector = tf.clip_by_value(pVector, 1e-9, 1.)
    # --- Compute functional to be minimized.
    return tf.reduce_mean(-tf.reduce_sum(yOverbar * tf.math.log(pVector), 1))

# --- Setting the Stochastic Gradient Descent Optimizer.
optimizer = tf.optimizers.SGD(alpha)

# --- Optimization step
def optimizationStep(features, classes):
    # --- Uses GradientTape for automatic differentiation
    with tf.GradientTape() as g:
        prob              = logisticRegression(features)
        costFunctionValue = costFunction(prob, classes)

    # --- Compute gradients
    gradients = g.gradient(costFunctionValue, [W, b])
    
    # --- Update the unknowns W and b
    optimizer.apply_gradients(zip(gradients, [W, b]))

# --- Accuracy metric function.
def accuracy(y_pred, yOverbar):
    # --- The predicted class is the index of highest score in the prediction vector (therefore, argmax).
    correctPrediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(yOverbar, tf.int64))
    return tf.reduce_mean(tf.cast(correctPrediction, tf.float32))

# --- Iterations loop
for iter, (featuresBatch, classesBatch) in enumerate(trainData.take(numIter), 1):

    optimizationStep(featuresBatch, classesBatch)
    
    if iter % skipIter == 0:
        prob              = logisticRegression(featuresBatch)
        costFunctionValue = costFunction(prob, classesBatch)
        print("iteration number: %i, cost function: %f" % (iter, costFunctionValue))

# Test model on validation set.
prob = logisticRegression(xTest)
print("Test Accuracy: %f" % accuracy(prob, yTest))

# --- Predict first images from validation set.
numImages = 5
testImages = xTest[:numImages]
predictions = logisticRegression(testImages)

# --- Comparison between images and model predictions.
for i in range(numImages):
    plt.imshow(np.reshape(testImages[i], [28, 28]), cmap='gray')
    plt.show()
    print("Model prediction: %i" % np.argmax(predictions.numpy()[i]))